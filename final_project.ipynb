{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readme(Important!!)\n",
    "\n",
    "Please also put the folder 'new_features' under the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import keras\n",
    "from keras import models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import glob\n",
    "from keras.layers import Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_data(df,targetdf):\n",
    "#     this is the method used to pre-process data. the main idea is to process values of each features \n",
    "#     for 5,7,9,...,79 different altitudes.\n",
    "#     and create a new feature csv file in shape(38,10) \n",
    "\n",
    "    radius =  3397200 + (targetdf[:,0]*1000)\n",
    "    radius_col = df[:,1]\n",
    "    new = pd.DataFrame([])\n",
    "    new = pd.concat((new,pd.DataFrame(targetdf[:,0])), axis=1)\n",
    "    for i in range(9):\n",
    "        y =  df[:,i+2]\n",
    "        intersections = pd.DataFrame(np.interp(radius, radius_col[::-1], y[::-1]))\n",
    "#         map values for altitudes\n",
    "        new = pd.concat((new,intersections[::-1]), axis=1)\n",
    "    new = new.reset_index(drop=True)\n",
    "    new.columns = [np.arange(0,new.shape[1])]\n",
    "    return new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob.glob('features/*.csv')\n",
    "count = 0\n",
    "# process all the feature files to create 1000 new feature files in name: new features\n",
    "mytarget = np.genfromtxt('targets/target_0001.csv',delimiter=',')\n",
    "for files in filenames:\n",
    "    count += 1;\n",
    "    myfeature = np.genfromtxt(files,delimiter=',');\n",
    "    aa = processing_data(myfeature,mytarget)\n",
    "    aa.to_csv(\"new_features/new_features{0000}.csv\".format(count),index=False,header=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Combine_Dataset(max,filenames,size):\n",
    "#     this method is to combine all the feature files or target files in a 3d array, first index is the index of file\n",
    "#     output_array[file_index][row_index][column_index]\n",
    "    cnt = 0;\n",
    "    output_array = np.zeros((max,38,size), dtype=np.float32)\n",
    "    for files in filenames:\n",
    "        myfeature = np.genfromtxt(files,delimiter=',');\n",
    "        output_array[cnt] = myfeature;\n",
    "        cnt += 1;\n",
    "        if(cnt >= max):\n",
    "            return output_array;\n",
    "    return output_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the feature and target dataset into 3 parts: \n",
    "# 800train, 100test, 100valid\n",
    "features = Combine_Dataset(1000,glob.glob('new_features/*.csv'),10)\n",
    "targets = Combine_Dataset(1000,glob.glob('targets/*.csv'),4)\n",
    "\n",
    "train_features, test_features = features[100:], features[:100]\n",
    "train_features, valid_features = train_features[100:], train_features[:100]\n",
    "\n",
    "train_targets, test_targets = targets[100:], targets[:100]\n",
    "train_targets, valid_targets = train_targets[100:], train_targets[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_65 (LSTM)               (None, 38, 256)           273408    \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 38, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_66 (LSTM)               (None, 38, 128)           197120    \n",
      "_________________________________________________________________\n",
      "lstm_67 (LSTM)               (None, 38, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_68 (LSTM)               (None, 38, 128)           131584    \n",
      "_________________________________________________________________\n",
      "lstm_69 (LSTM)               (None, 38, 64)            49408     \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 38, 64)            0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 38, 4)             260       \n",
      "=================================================================\n",
      "Total params: 783,364\n",
      "Trainable params: 783,364\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build LSTM model for learning\n",
    "# outputdim for first layer is 256,input shape is (38,10)\n",
    "model = Sequential()\n",
    "model.add(LSTM(256,input_shape=(38,10),return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128, return_sequences = True))\n",
    "model.add(LSTM(128, return_sequences = True))\n",
    "model.add(LSTM(128, return_sequences = True))\n",
    "model.add(LSTM(64, return_sequences = True))\n",
    "\n",
    "# model.add(LSTM(38, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(LSTM(units = 64, return_sequences = False))\n",
    "model.add(Dense(4))\n",
    "\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'mse',metrics = ['mse'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "80/80 [==============================] - 10s 76ms/step - loss: 1326.1694 - mse: 1326.1693 - val_loss: 1062.8391 - val_mse: 1062.8391\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - 5s 60ms/step - loss: 1062.7184 - mse: 1062.7187 - val_loss: 932.0852 - val_mse: 932.0854\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - 5s 62ms/step - loss: 941.6414 - mse: 941.6415 - val_loss: 832.5050 - val_mse: 832.5049\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - 5s 61ms/step - loss: 841.2733 - mse: 841.2734 - val_loss: 758.5745 - val_mse: 758.5745\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - 5s 61ms/step - loss: 781.2918 - mse: 781.2918 - val_loss: 702.1830 - val_mse: 702.1830\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - 5s 63ms/step - loss: 722.4054 - mse: 722.4054 - val_loss: 651.0735 - val_mse: 651.0735\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - 5s 63ms/step - loss: 680.3062 - mse: 680.3063 - val_loss: 610.5486 - val_mse: 610.5487\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - 5s 63ms/step - loss: 636.3700 - mse: 636.3700 - val_loss: 575.6566 - val_mse: 575.6566\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - 5s 60ms/step - loss: 599.6547 - mse: 599.6548 - val_loss: 549.0035 - val_mse: 549.0035\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - 5s 59ms/step - loss: 571.1039 - mse: 571.1038 - val_loss: 524.4608 - val_mse: 524.4608\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - 5s 59ms/step - loss: 552.6254 - mse: 552.6254 - val_loss: 491.7180 - val_mse: 491.7180\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - 5s 59ms/step - loss: 516.5675 - mse: 516.5676 - val_loss: 464.8205 - val_mse: 464.8205\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - 5s 59ms/step - loss: 494.6693 - mse: 494.6693 - val_loss: 440.3292 - val_mse: 440.3292\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - 5s 59ms/step - loss: 462.4875 - mse: 462.4875 - val_loss: 418.1375 - val_mse: 418.1375\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - 5s 60ms/step - loss: 434.9530 - mse: 434.9531 - val_loss: 394.5076 - val_mse: 394.5076\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - 5s 59ms/step - loss: 422.2050 - mse: 422.2050 - val_loss: 374.2021 - val_mse: 374.2022\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - 5s 59ms/step - loss: 399.6535 - mse: 399.6535 - val_loss: 354.7742 - val_mse: 354.7742\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - 5s 59ms/step - loss: 380.1728 - mse: 380.1729 - val_loss: 333.5473 - val_mse: 333.5472\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - 5s 59ms/step - loss: 356.7630 - mse: 356.7630 - val_loss: 317.0537 - val_mse: 317.0537\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - 5s 61ms/step - loss: 332.1397 - mse: 332.1397 - val_loss: 307.8184 - val_mse: 307.8184\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - 5s 62ms/step - loss: 312.1745 - mse: 312.1745 - val_loss: 271.8401 - val_mse: 271.8401\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - 5s 61ms/step - loss: 289.6402 - mse: 289.6402 - val_loss: 255.0054 - val_mse: 255.0054\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - 5s 61ms/step - loss: 267.7807 - mse: 267.7807 - val_loss: 240.1688 - val_mse: 240.1688\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - 6s 70ms/step - loss: 255.2232 - mse: 255.2233 - val_loss: 224.0818 - val_mse: 224.0818\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - 6s 81ms/step - loss: 237.8160 - mse: 237.8161 - val_loss: 211.6819 - val_mse: 211.6820\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - 6s 81ms/step - loss: 224.0067 - mse: 224.0068 - val_loss: 198.2866 - val_mse: 198.2866\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 211.7987 - mse: 211.7987 - val_loss: 185.1854 - val_mse: 185.1854\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - 7s 82ms/step - loss: 195.0902 - mse: 195.0903 - val_loss: 173.4767 - val_mse: 173.4767\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - 6s 81ms/step - loss: 183.6665 - mse: 183.6665 - val_loss: 161.8609 - val_mse: 161.8608\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 171.2950 - mse: 171.2950 - val_loss: 160.0286 - val_mse: 160.0286\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 166.5648 - mse: 166.5648 - val_loss: 139.8724 - val_mse: 139.8724\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 149.2625 - mse: 149.2625 - val_loss: 133.4498 - val_mse: 133.4498\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 141.9194 - mse: 141.9194 - val_loss: 128.1591 - val_mse: 128.1591\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 135.5150 - mse: 135.5150 - val_loss: 117.4774 - val_mse: 117.4774\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 121.6940 - mse: 121.6940 - val_loss: 118.4417 - val_mse: 118.4417\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 121.0097 - mse: 121.0097 - val_loss: 111.2644 - val_mse: 111.2644\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 116.7796 - mse: 116.7796 - val_loss: 107.2968 - val_mse: 107.2968\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 106.2633 - mse: 106.2633 - val_loss: 101.5335 - val_mse: 101.5335\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 102.4567 - mse: 102.4567 - val_loss: 93.6512 - val_mse: 93.6512\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 100.4958 - mse: 100.4958 - val_loss: 97.6626 - val_mse: 97.6626\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 99.9647 - mse: 99.9648 - val_loss: 88.8205 - val_mse: 88.8205\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 95.4711 - mse: 95.4711 - val_loss: 85.5338 - val_mse: 85.5338\n",
      "Epoch 43/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 86.7884 - mse: 86.7884 - val_loss: 82.6765 - val_mse: 82.6765\n",
      "Epoch 44/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 85.8054 - mse: 85.8054 - val_loss: 80.9139 - val_mse: 80.9139\n",
      "Epoch 45/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 80.5793 - mse: 80.5793 - val_loss: 84.7514 - val_mse: 84.7514\n",
      "Epoch 46/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 79.5535 - mse: 79.5535 - val_loss: 78.5759 - val_mse: 78.5758\n",
      "Epoch 47/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 84.1555 - mse: 84.1555 - val_loss: 77.1245 - val_mse: 77.1245\n",
      "Epoch 48/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 73.9415 - mse: 73.9415 - val_loss: 78.0810 - val_mse: 78.0810\n",
      "Epoch 49/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 77.9221 - mse: 77.9221 - val_loss: 75.6674 - val_mse: 75.6674\n",
      "Epoch 50/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 73.0032 - mse: 73.0032 - val_loss: 80.5922 - val_mse: 80.5922\n",
      "Epoch 51/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 73.0955 - mse: 73.0955 - val_loss: 74.5003 - val_mse: 74.5003\n",
      "Epoch 52/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 78.6620 - mse: 78.6620 - val_loss: 74.6870 - val_mse: 74.6870\n",
      "Epoch 53/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 69.5115 - mse: 69.5115 - val_loss: 73.8247 - val_mse: 73.8247\n",
      "Epoch 54/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 76.2060 - mse: 76.2060 - val_loss: 74.5176 - val_mse: 74.5176\n",
      "Epoch 55/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 66.9511 - mse: 66.9511 - val_loss: 74.3047 - val_mse: 74.3047\n",
      "Epoch 56/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 69.6923 - mse: 69.6923 - val_loss: 76.7433 - val_mse: 76.7433\n",
      "Epoch 57/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 73.9216 - mse: 73.9216 - val_loss: 73.5442 - val_mse: 73.5442\n",
      "Epoch 58/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 68.0363 - mse: 68.0363 - val_loss: 73.2933 - val_mse: 73.2933\n",
      "Epoch 59/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 77.5488 - mse: 77.5488 - val_loss: 73.5460 - val_mse: 73.5460\n",
      "Epoch 60/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 74.4617 - mse: 74.4616 - val_loss: 73.8396 - val_mse: 73.8396\n",
      "Epoch 61/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 69.6706 - mse: 69.6706 - val_loss: 74.0494 - val_mse: 74.0494\n",
      "Epoch 62/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 68.5546 - mse: 68.5546 - val_loss: 78.1332 - val_mse: 78.1332\n",
      "Epoch 63/100\n",
      "80/80 [==============================] - 7s 83ms/step - loss: 67.5424 - mse: 67.5424 - val_loss: 73.4435 - val_mse: 73.4435\n",
      "Epoch 64/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 65.0082 - mse: 65.0082 - val_loss: 73.3433 - val_mse: 73.3433\n",
      "Epoch 65/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 71.6997 - mse: 71.6997 - val_loss: 76.0309 - val_mse: 76.0309\n",
      "Epoch 66/100\n",
      "80/80 [==============================] - 6s 81ms/step - loss: 73.6063 - mse: 73.6063 - val_loss: 75.8801 - val_mse: 75.8801\n",
      "Epoch 67/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 65.2682 - mse: 65.2682 - val_loss: 77.2815 - val_mse: 77.2815\n",
      "Epoch 68/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 69.5288 - mse: 69.5288 - val_loss: 74.1340 - val_mse: 74.1340\n",
      "Epoch 69/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 63.7469 - mse: 63.7469 - val_loss: 78.9414 - val_mse: 78.9414\n",
      "Epoch 70/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 66.4784 - mse: 66.4784 - val_loss: 76.5079 - val_mse: 76.5079\n",
      "Epoch 71/100\n",
      "80/80 [==============================] - 6s 81ms/step - loss: 73.5439 - mse: 73.5439 - val_loss: 74.0064 - val_mse: 74.0064\n",
      "Epoch 72/100\n",
      "80/80 [==============================] - 6s 81ms/step - loss: 71.5363 - mse: 71.5363 - val_loss: 74.8693 - val_mse: 74.8693\n",
      "Epoch 73/100\n",
      "80/80 [==============================] - 6s 81ms/step - loss: 65.8580 - mse: 65.8580 - val_loss: 73.3802 - val_mse: 73.3802\n",
      "Epoch 74/100\n",
      "80/80 [==============================] - 6s 81ms/step - loss: 65.5341 - mse: 65.5341 - val_loss: 73.4799 - val_mse: 73.4799\n",
      "Epoch 75/100\n",
      "80/80 [==============================] - 6s 81ms/step - loss: 70.7075 - mse: 70.7075 - val_loss: 73.5041 - val_mse: 73.5041\n",
      "Epoch 76/100\n",
      "80/80 [==============================] - 6s 81ms/step - loss: 64.9554 - mse: 64.9554 - val_loss: 74.7051 - val_mse: 74.7051\n",
      "Epoch 77/100\n",
      "80/80 [==============================] - 6s 81ms/step - loss: 71.2069 - mse: 71.2069 - val_loss: 74.6577 - val_mse: 74.6577\n",
      "Epoch 78/100\n",
      "80/80 [==============================] - 7s 82ms/step - loss: 74.0312 - mse: 74.0312 - val_loss: 79.7949 - val_mse: 79.7949\n",
      "Epoch 79/100\n",
      "80/80 [==============================] - 6s 81ms/step - loss: 71.3642 - mse: 71.3642 - val_loss: 73.8389 - val_mse: 73.8389\n",
      "Epoch 80/100\n",
      "80/80 [==============================] - 7s 82ms/step - loss: 71.6076 - mse: 71.6076 - val_loss: 73.5081 - val_mse: 73.5081\n",
      "Epoch 81/100\n",
      "80/80 [==============================] - 7s 81ms/step - loss: 66.8552 - mse: 66.8552 - val_loss: 72.8686 - val_mse: 72.8686\n",
      "Epoch 82/100\n",
      "80/80 [==============================] - 7s 84ms/step - loss: 72.2066 - mse: 72.2066 - val_loss: 76.7682 - val_mse: 76.7682\n",
      "Epoch 83/100\n",
      "80/80 [==============================] - 7s 83ms/step - loss: 72.4755 - mse: 72.4755 - val_loss: 74.2690 - val_mse: 74.2690\n",
      "Epoch 84/100\n",
      "80/80 [==============================] - 7s 83ms/step - loss: 71.3597 - mse: 71.3597 - val_loss: 74.2048 - val_mse: 74.2048\n",
      "Epoch 85/100\n",
      "80/80 [==============================] - 7s 82ms/step - loss: 70.6298 - mse: 70.6298 - val_loss: 76.4224 - val_mse: 76.4224\n",
      "Epoch 86/100\n",
      "80/80 [==============================] - 7s 82ms/step - loss: 70.1577 - mse: 70.1577 - val_loss: 73.9936 - val_mse: 73.9936\n",
      "Epoch 87/100\n",
      "80/80 [==============================] - 6s 81ms/step - loss: 65.4222 - mse: 65.4222 - val_loss: 73.7389 - val_mse: 73.7389\n",
      "Epoch 88/100\n",
      "80/80 [==============================] - 6s 81ms/step - loss: 62.7813 - mse: 62.7813 - val_loss: 73.4347 - val_mse: 73.4347\n",
      "Epoch 89/100\n",
      "80/80 [==============================] - 7s 83ms/step - loss: 68.1011 - mse: 68.1011 - val_loss: 73.6837 - val_mse: 73.6837\n",
      "Epoch 90/100\n",
      "80/80 [==============================] - 7s 81ms/step - loss: 70.0370 - mse: 70.0370 - val_loss: 75.0834 - val_mse: 75.0834\n",
      "Epoch 91/100\n",
      "80/80 [==============================] - 6s 81ms/step - loss: 67.1945 - mse: 67.1945 - val_loss: 73.3899 - val_mse: 73.3899\n",
      "Epoch 92/100\n",
      "80/80 [==============================] - 7s 81ms/step - loss: 67.9573 - mse: 67.9573 - val_loss: 73.6454 - val_mse: 73.6454\n",
      "Epoch 93/100\n",
      "80/80 [==============================] - 7s 82ms/step - loss: 65.3788 - mse: 65.3788 - val_loss: 74.0089 - val_mse: 74.0089\n",
      "Epoch 94/100\n",
      "80/80 [==============================] - 7s 82ms/step - loss: 67.7457 - mse: 67.7457 - val_loss: 72.9691 - val_mse: 72.9691\n",
      "Epoch 95/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 65.1135 - mse: 65.1135 - val_loss: 73.0846 - val_mse: 73.0846\n",
      "Epoch 96/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 67.0900 - mse: 67.0900 - val_loss: 73.5094 - val_mse: 73.5094\n",
      "Epoch 97/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 67.6158 - mse: 67.6158 - val_loss: 73.7357 - val_mse: 73.7357\n",
      "Epoch 98/100\n",
      "80/80 [==============================] - 6s 80ms/step - loss: 67.7532 - mse: 67.7532 - val_loss: 159.1278 - val_mse: 159.1278\n",
      "Epoch 99/100\n",
      "80/80 [==============================] - 6s 79ms/step - loss: 98.5090 - mse: 98.5090 - val_loss: 73.5125 - val_mse: 73.5125\n",
      "Epoch 100/100\n",
      "80/80 [==============================] - 6s 79ms/step - loss: 64.3534 - mse: 64.3534 - val_loss: 73.9956 - val_mse: 73.9956\n"
     ]
    }
   ],
   "source": [
    "#feed data to the LSTM,\n",
    "history = model.fit(train_features,train_targets,batch_size=10,validation_data=(valid_features,valid_targets), epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'epochs')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(len(history.history['loss']))\n",
    "f,fig=plt.subplots( figsize=(10,4))\n",
    "# fig[0].plot  ( epochs,     history.history[     'loss' ], label='Training')\n",
    "# fig[0].plot  ( epochs, history.history[     'val_loss' ], label='Validation')\n",
    "# plt.title ('Training and validation loss')\n",
    "# fig[0].legend()\n",
    "# plt.xlabel('epochs')\n",
    "\n",
    "fig.plot  ( epochs,     history.history[     'mse' ], label='Training')\n",
    "fig.plot  ( epochs, history.history[     'val_mse' ], label='Validation')\n",
    "# plt.legend()\n",
    "plt.title ('Training and validation mse')\n",
    "plt.xlabel('epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "density error:  6.138885852554256 %,\n",
      "wind error: 4.145183919633779 % \n",
      "final_err= 7.2218773188980165 %\n"
     ]
    }
   ],
   "source": [
    "#this code allows you to calculate final percentage error for one trajectory. \n",
    "#You can use it to iterate over whole testing dataset and average the final error over all trajectories in it\n",
    "# model.save(\"LSTM_Model\")\n",
    "loaded = keras.models.load_model(\"LSTM_Model\")\n",
    "\n",
    "final_err= []\n",
    "for i in range(100):\n",
    "    xx=np.reshape(features[i],(-1,38,10))\n",
    "    yy=np.reshape(targets[i],(-1,38,4))\n",
    "\n",
    "    predictions= loaded.predict(xx)  #a 38x4 numpy array with predicted atmospehric conditions returned by your model\n",
    "    ground_truth=yy  #a 38x4 numpy array from the target file corresponding to features used in the predictions\n",
    "\n",
    "    diffwind=np.sqrt(np.square(predictions[:,3]-ground_truth[:,3])+np.square(predictions[:,2]-ground_truth[:,3])) #calculate vector difference in wind\n",
    "    windvecGT=np.sqrt(np.square(ground_truth[:,3])+np.square(ground_truth[:,2])) #length of the ground_truth wind vector at each altitude\n",
    "\n",
    "    #relative (percentage) errors for each altitude\n",
    "    rel_err_d=100*(predictions[:,1]-ground_truth[:,1])/ground_truth[:,1]\n",
    "    rel_err_w=100*(diffwind)/windvecGT\n",
    "\n",
    "    #RMS percentage errors for the whole trajectory, density and wind separately\n",
    "    RMS_d=np.sqrt(1/38*np.sum(np.square(rel_err_d)))\n",
    "    RMS_w=np.sqrt(1/38*np.sum(np.square(rel_err_w)))\n",
    "    final_err.append((RMS_d+RMS_w)/2)  #final error, average of all variables\n",
    "\n",
    "final_err= np.mean(final_err)\n",
    "print('density error: ',RMS_d,'%,\\nwind error:', RMS_w, '% \\nfinal_err=', final_err, '%')\n",
    "\n",
    "# print(final_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the cell below, for one click prediction!!\n",
    "it is a section that combines the functions above, for convinience only\n",
    "## Before run the cell below, please make sure put the 'new_features' folder in the directory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "density error:  6.138885852554256 %,\n",
      "wind error: 4.145183919633779 % \n",
      "final_err= 7.2218773188980165 %\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import keras\n",
    "from keras import models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "import glob\n",
    "from keras.layers import Reshape\n",
    "\n",
    "def Combine_Dataset(max,filenames,size):\n",
    "#     this method is to combine all the feature files or target files in a 3d array, first index is the index of file\n",
    "#     output_array[file_index][row_index][column_index]\n",
    "    cnt = 0;\n",
    "    output_array = np.zeros((max,38,size), dtype=np.float32)\n",
    "    for files in filenames:\n",
    "        myfeature = np.genfromtxt(files,delimiter=',');\n",
    "        output_array[cnt] = myfeature;\n",
    "        cnt += 1;\n",
    "        if(cnt >= max):\n",
    "            return output_array;\n",
    "    return output_array\n",
    "# data splitting\n",
    "#split the feature and target dataset into 3 parts: \n",
    "# 800train, 100test, 100valid\n",
    "features = Combine_Dataset(1000,glob.glob('new_features/*.csv'),10)\n",
    "targets = Combine_Dataset(1000,glob.glob('targets/*.csv'),4)\n",
    "\n",
    "train_features, test_features = features[100:], features[:100]\n",
    "train_features, valid_features = train_features[100:], train_features[:100]\n",
    "\n",
    "train_targets, test_targets = targets[100:], targets[:100]\n",
    "train_targets, valid_targets = train_targets[100:], train_targets[:100]\n",
    "\n",
    "loaded = keras.models.load_model(\"LSTM_Model\")\n",
    "\n",
    "final_err= []\n",
    "for i in range(100):\n",
    "    xx=np.reshape(features[i],(-1,38,10))\n",
    "    yy=np.reshape(targets[i],(-1,38,4))\n",
    "\n",
    "    predictions= loaded.predict(xx)  #a 38x4 numpy array with predicted atmospehric conditions returned by your model\n",
    "    ground_truth=yy  #a 38x4 numpy array from the target file corresponding to features used in the predictions\n",
    "\n",
    "    diffwind=np.sqrt(np.square(predictions[:,3]-ground_truth[:,3])+np.square(predictions[:,2]-ground_truth[:,3])) #calculate vector difference in wind\n",
    "    windvecGT=np.sqrt(np.square(ground_truth[:,3])+np.square(ground_truth[:,2])) #length of the ground_truth wind vector at each altitude\n",
    "\n",
    "    #relative (percentage) errors for each altitude\n",
    "    rel_err_d=100*(predictions[:,1]-ground_truth[:,1])/ground_truth[:,1]\n",
    "    rel_err_w=100*(diffwind)/windvecGT\n",
    "\n",
    "    #RMS percentage errors for the whole trajectory, density and wind separately\n",
    "    RMS_d=np.sqrt(1/38*np.sum(np.square(rel_err_d)))\n",
    "    RMS_w=np.sqrt(1/38*np.sum(np.square(rel_err_w)))\n",
    "    final_err.append((RMS_d+RMS_w)/2)  #final error, average of all variables\n",
    "\n",
    "final_err= np.mean(final_err)\n",
    "print('density error: ',RMS_d,'%,\\nwind error:', RMS_w, '% \\nfinal_err=', final_err, '%')\n",
    "\n",
    "# print(final_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
